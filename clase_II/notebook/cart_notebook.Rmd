---
title: "Implementando modelos basados en árboles en `caret`"
author: "Germán Rosati"
output: html_notebook
---

## Objetivos

- Introducir los principales conceptos alrededor de la estimación de modelos basados en árboles de decisión (cart, bagging, random forest) 
- Mostrar su implementación en `caret`


## El problema

Segumos con nuestro problema central: predecir los ingresos de la ocupación principal (`p21`) en la EPH del segundo trimestre del 2015. Pero en esta oportunidad queremos evaluar si podemos predecir la no respuesta. Es decir, entrenar un modelo que nos permita predecir qué tan probable es que una persona NO responda ingresos.

Ya hemos preprocesado los datos y estamos listos 

Lo primero que tenemos que hacer es importar las librerías con las que vamos a trabajar:


```{r, message=FALSE}
library(caret)
library(tidyverse)
library(rpart)
```


Luego, cargamos los datos y formateamos un poco algunas etiquetas:


```{r}
load('../data/EPH_2015_II.RData')

data$pp03i<-factor(data$pp03i, labels=c('1-SI', '2-No', '9-NS'))



data$intensi<-factor(data$intensi, labels=c('1-Sub_dem', '2-SO_no_dem', 
                                            '3-Ocup.pleno', '4-Sobreoc',
                                            '5-No trabajo', '9-NS'))

data$pp07a<-factor(data$pp07a, labels=c('0-NC',
                                        '1-Menos de un mes',
                                        '2-1 a 3 meses',
                                        '3-3 a 6 meses',
                                        '4-6 a 12 meses',
                                        '5-12 a 60 meses',
                                        '6-Más de 60 meses',
                                        '9-NS'))



data <- data %>%
        mutate(imp_inglab1=factor(imp_inglab1, labels=c('non_miss','miss')))
```

Ahora, nuestra variable a predecir es el indicador `imp_inglab`. Por ende, elimonamos la $p21$.

```{r}

df_train <- data %>%
        select(-p21)

```

Lo primero que vamos a hacer es crear una partición de datos:

```{r}
set.seed(1234)
tr_index <- createDataPartition(y=df_train$imp_inglab1,
                                p=0.8,
                                list=FALSE)
```

Y generamos dos datasets

```{r}
train <- df_train[tr_index,]
test <- df_train[-tr_index,]
```



## Entrenando modelos (`train()`)
### CART -  Classification and Regression Trees

Empecemos por entrenar algunos árboles simples para tener una idea del proceso. Para entrenar modelos sin tunear hiperparámetros, tenemos que definir un objeto `trainControl` con `method='none'`.


```{r}
fitControl <- trainControl(method = "none", classProbs = FALSE)
```

Ahora podemos entrenar un árbol poco profundo... digamos, 3. 

```{r}
cart_tune <- train(imp_inglab1 ~ . , 
                 data = df_train, 
                 method = "rpart2", 
                 trControl = fitControl,
                 tuneGrid = data.frame(maxdepth=3),
                 control = rpart.control(minsplit = 1,
                                         minbucket = 1,
                                        cp=0.00000001)
)
```

Y podemos plotearlo de forma fea:

```{r}
plot(cart_tune$finalModel)
text(cart_tune$finalModel, pretty=1)
```

O de forma bonita:

```{r}
library(rpart.plot)
```

```{r}
rpart.plot(cart_tune$finalModel)
```


Testeemos la performance de este árbol:

```{r}
table(predict(cart_tune, df_train) , df_train$imp_inglab1)
```

¿Qué conclusión pueden sacar al respecto?

Entrenen, ahora, un segundo árbol pero más complejo: `maxdepth=10`.


```{r}
cart_tune <- train(imp_inglab1 ~ . , 
                 data = df_train, 
                 method = "rpart2", 
                 trControl = fitControl,
                 tuneGrid = data.frame(maxdepth=10),
                 control = rpart.control(cp=0.0001)
)
```

```{r}
rpart.plot(cart_tune$finalModel)
```

```{r}
table(predict(cart_tune, df_train) , df_train$imp_inglab1)
```

Hasta aquí estuvimos haciendo trampa. Vamos a ahora a tunear el parámetro de profundidad de forma correcta.


### Seteando la partición para evaluar

Primero, fijamos la semilla aleatoria (para asegurarnos la posibilidad de replicabilidad)


```{r}
set.seed(789)
```

Podemos usar la función `createFolds()` para generar los índices. Aquí, pas

```{r}
cv_index <- createFolds(y = train$imp_inglab1,
                        k=5,
                        list=TRUE,
                        returnTrain=TRUE)
```

Finalmente, especificamos el diseño de remuestreo mediante la función `trainControl`:

```{r}
fitControl <- trainControl(
        index=cv_index,
        method="cv",
        number=5
        )
```


```{r}
grid <- expand.grid(maxdepth=c(1, 2, 4, 8, 10, 15, 20))
```

Y volvemos a entrenar el modelo:

```{r warning=FALSE}
cart_tune <- train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "rpart2", 
                 trControl = fitControl,
                 tuneGrid = grid,
                 control = rpart.control(cp=0.000001)
)

cart_tune
```


## Seleccionando el mejor modelo

Una vez finalizado el proceso de tunning de los hiperparámetros, podemos proceder a elegir cuál es el mejor modelo y entrenarlo sobre todo el dataset. Podemos ver que el mejor es un árbol que parece demasiado complejo `maxdepth=15`, por ello, vamos a elegir uno un poco más interpreable

```{r}
cart_final <- train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "rpart2", 
                 tuneGrid = data.frame(maxdepth=6),
                 control = rpart.control(cp=0.000001)
)
```

Podemos visualizarlo:

```{r fig.height=12, fig.width=20}
rpart.plot(cart_final$finalModel)
```


Y generamos las predicciones finales:

```{r}
y_preds <- predict(cart_final, test)
```

Generamos nuestra matriz de confusión:

```{r}
confusionMatrix(y_preds, test$imp_inglab1)
```

¿Qué se puede decir del árbol de decisión? ¿Cómo funciona?

