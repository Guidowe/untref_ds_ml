---
title: "Ensamble Learning - Bagging, Random Forest y AdaBoost"
author: "Germán Rosati"
output: html_notebook
---

## Objetivos

- Mostrar la implementación en `caret` de diferentes algoritmos de Ensamble Learning para problemas de clasificación
- Mostrar los problemas de los datasets desbalanceados y algunos métodos posibles para lidiar con el problema

## Carga de librerías y datos

```{r, message=FALSE}
library(caret)
library(tidyverse)
library(rpart)
```


Luego, cargamos los datos y formateamos un poco algunas etiquetas:


```{r}
load('../data/EPH_2015_II.RData')

data$pp03i<-factor(data$pp03i, labels=c('1-SI', '2-No', '9-NS'))



data$intensi<-factor(data$intensi, labels=c('1-Sub_dem', '2-SO_no_dem', 
                                            '3-Ocup.pleno', '4-Sobreoc',
                                            '5-No trabajo', '9-NS'))

data$pp07a<-factor(data$pp07a, labels=c('0-NC',
                                        '1-Menos de un mes',
                                        '2-1 a 3 meses',
                                        '3-3 a 6 meses',
                                        '4-6 a 12 meses',
                                        '5-12 a 60 meses',
                                        '6-Más de 60 meses',
                                        '9-NS'))



data <- data %>%
        mutate(imp_inglab1=factor(imp_inglab1, labels=c('non_miss','miss')))
```

Ahora, nuestra variable a predecir es el indicador `imp_inglab`. Por ende, elimonamos la $p21$.

```{r}

df_train <- data %>%
        select(-p21)

```

Lo primero que vamos a hacer es crear una partición de datos:

```{r}
set.seed(1234)
tr_index <- createDataPartition(y=df_train$imp_inglab1,
                                p=0.8,
                                list=FALSE)
```

Y generamos dos datasets

```{r}
train <- df_train[tr_index,]
test <- df_train[-tr_index,]
```


## Random Forest

La idea ahora es poder entrenar un modelo random forest. Pero... lo van a hacer ustedes. Para ello, deberán consultar [la documentación de caret](http://topepo.github.io/caret/available-models.html) y buscar cuál es la mejor función para entrenar el modelo y sus hiperparámetros.

Pista: buscar el método `ranger`

Usaremos la partición original entre Train-Test. Recordemos los pasos a seguir

1. Generar el esquema de validación cruzada
2. Generar el grid de hiperparámetros
3. Tunear el modelo
4. Seleccionar y estimar el modelo final
5. Validar y evaluar contra test-set


```{r}
set.seed(5699)
cv_index_rf <- createFolds(y=train$imp_inglab1,
                        k=5,
                        list=TRUE,
                        returnTrain=TRUE)

fitControlrf <- trainControl(
        index=cv_index_rf,
        method="cv",
        number=5,
        summaryFunction = twoClassSummary,
        classProbs=TRUE
        )

# Generar grid

grid_rf <- expand.grid(mtry=c(5,10,15, 25),
                       min.node.size=c(5,10,15,20),
                       splitrule='gini'
        )

```


```{r eval=FALSE, include=TRUE}
# Tunear
t0 <- proc.time()
rf_fit_orig <-  train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "ranger", 
                 trControl = fitControlrf,
                 tuneGrid = grid_rf,
                 metric='ROC'
)
proc.time() -  t0


saveRDS(rf_fit_orig, '../models/rf_fit_orig.RDS')
```

```{r include=FALSE}
rf_fit_orig <- readRDS('../models/rf_fit_orig.RDS')
```

```{r}
rf_fit_orig
```


```{r}
confusionMatrix(predict(rf_fit_orig, test), test$imp_inglab1)
```


- ¿Qué pueden decir de este modelo? 
- ¿Funciona mejor que un árbol individual? 
- ¿Qué sucede con su performance entre los `miss`


## Datasets desbalanceados... ¿qué hacer?

Efectivamente, parte del problema de este dataset es que tenemos una variable dependiente sumamente desbalanceada:

```{r}
ggplot(train) + 
        geom_bar(aes(x=imp_inglab1))
```

#### Matriz de confusión

Ya estamos familiarizades con la matriz de confusión. Básicamente, es una forma rápida de comparar las predicciones del modelo con los valores reales. 

Así, la métrica más simple es la siguiente:

$Accuracy = \frac{TP + TN}{N} = \frac{TP + TN}{TP + TN + FP + FN}$

Simplemente, calcula la proporción de casos bien clasificados ($TP + TN$) sobre el total. Pero esta métrica tiene algunos problemas. El más importante es que tiende a ser una medida engañosa de la performance de un modelo en un dataset desbalanceado.

Un primer punto importante es poder seleccionar una métrica adecuada para evaluar nuestro modelo. Existen dos grandes familias de medidas:

- Dependiente del umbral: Esto incluye métricas como precisión, recall y puntaje F1, que requieren una matriz de confusión para ser calculada usando un límite duro en las probabilidades pronosticadas. Estas métricas suelen ser bastante pobres en el caso de clases desequilibradas, ya que el software estadístico utiliza de manera inapropiada un umbral predeterminado de 0,50, lo que hace que el modelo prediga que todas las observaciones pertenecen a la clase mayoritaria.

$Precision = \frac{TP}{TP+FP}$ Ratio entre verdaderos positivos y los que el modelo  clasifica como positivos. ¿Qué proporción de los que clasifcamos como positivos son efectivamente positivos?

$Recall =  \frac{TP}{TP+FN}$ También llamada, $Sensitividad$, Ratio entre los positivos bien clasificados y el total de los que son positivos en realidad. ¿Qué proporción de los que efectivamente positivos hemos clasificado bien?

$F1-Score = 2 \times \frac{Precision \times Recall}{Recall + Precision}$

- Independiente del umbral: Esto incluye métricas como área bajo la curva ROC (AUC), que cuantifica la tasa positiva verdadera en función de la tasa de falsos positivos para una variedad de umbrales de clasificación. Otra forma de interpretar esta métrica es la probabilidad de que una instancia positiva aleatoria tenga una probabilidad estimada más alta que una instancia negativa aleatoria.

En líneas generales, existen cuatro formas generales de lidiar con el problema de las clases desbalanceadas:

- Usar modelos ponderados
- _Down-sampling_ es decir, submuestrear las instancias sobrerrepresentadas
- _Up-sampling_, es decir, sobremuestrar las instancias subrrepresentadas
- _Synthetic minority sampling technique (SMOTE)_: hace _down-sampling_ de la clase mayoritaria y "sintetiza" nuevas instancias minoritarias mediante interpolación


### Utilizar pesos en el modelo

Para esto podemos utilizar el argumento `weights` en la función `train` -esto supone que el modelo seleccionado puede manejar pesos-...


```{r}
model_weights <- ifelse(train$imp_inglab1 == "non_miss",
                        (1/table(train$imp_inglab1)[1]) * 0.5,
                        (1/table(train$imp_inglab1)[2]) * 0.5)

fitControlrf$seeds <- rf_fit_orig$control$seeds
```


Y entrenamos el modelo:

```{r}
t0 <- proc.time()
rf_fit_wei <-  train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "ranger", 
                 trControl = fitControlrf,
                 tuneGrid = grid_rf,
                 weights = model_weights,
                 metric='ROC'
)
proc.time() -  t0

#saveRDS(rf_fit_wei, '../models/rf_fit_wei.RDS')

```

```{r include=FALSE}
rf_fit_wei <- readRDS('../models/rf_fit_wei.RDS')
```


### Métodos de remuestreo

Probemos, ahora, métodos de remuestreo: _up sampling_ y _down sampling_.

```{r}

# Nuevo fitControl con upsample

fitControlrf_imb <- trainControl(
        index=cv_index_rf,
        method="cv",
        number=5,
        summaryFunction = twoClassSummary,
        classProbs=TRUE,
        sampling = 'up'
        )

```

```{r eval=FALSE, include=TRUE}

# Tunear upsample

fitControlrf_imb$seeds <- rf_fit_orig$control$seeds

t0 <- proc.time()
rf_fit_up <-  train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "ranger", 
                 trControl = fitControlrf_imb,
                 tuneGrid = grid_rf,
                 metric='ROC'
)
proc.time() -  t0

saveRDS(rf_fit_up, '../models/rf_fit_up.RDS')
```

```{r include=FALSE}
rf_fit_up <- readRDS('../models/rf_fit_up.RDS')

```



```{r eval=FALSE, include=TRUE}
# Tunear downsample

fitControlrf_imb$sampling <- "down"
fitControlrf_imb$seeds <- rf_fit_orig$control$seeds


t0 <- proc.time()
rf_fit_down <-  train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "ranger", 
                 trControl = fitControlrf_imb,
                 tuneGrid = grid_rf,
                 metric='ROC'
)
proc.time() -  t0

#saveRDS(rf_fit_down, '../models/rf_fit_down.RDS')

```

```{r include=FALSE}
rf_fit_down <- readRDS('../models/rf_fit_down.RDS')
```


Comparemos todos los modelos entrenados hasta ahora. Pero ya que estamos, incorporemos el modelo de árbol simple que habíamos entrenado la semana pasada:


```{r}
cart <- readRDS('../models/cart.RDS')
```


Colocamos todos los modelos en una lista:

```{r}
model_list <- list(cart = cart,
                   rf_original = rf_fit_orig,
                   rf_weighted = rf_fit_wei,
                   rf_down = rf_fit_down,
                   rf_up = rf_fit_up)

```


Extraigamos algunas métricas resumen de las matrices de confusión:

```{r}
extract_conf_metrics <- function(model, data, obs){
        preds <- predict(model, data)        
        c<-confusionMatrix(preds, obs)
        results <- c(c$overall[1], c$byClass)
        return(results)
}

```


```{r eval=FALSE, include=TRUE}

model_metrics <- model_list %>%
        map(extract_conf_metrics, data=test, obs = test$imp_inglab1) %>%
        do.call(rbind,.) %>%
        as.data.frame()

```

Puede verse, entonces, cómo la performace de los diferentes modelos cambia en los diferentes tunnings...

```{r}
model_metrics %>%
        select('Accuracy', 'Precision', 'Recall') %>%
        t()
```

Observando específicamente las métricas de $Precision$, $Recall$ y $Accuracy$ puede verse que el modelo de random forest original parece mejorar las diferentes métricas.


## AdaBoost

Por último, vamos a entrenar un modelo de Adaboost.

```{r}
fitControlrf$seeds <- rf_fit_orig$control$seeds
#grid_ada <- expand.grid(mfinal=c(100,150), maxdepth=c(10,20), coeflearn='Breiman')
grid_ada <- expand.grid(nIter=c(100,150), method='Adaboost.M1')
```

```{r include=TRUE}
t0 <- proc.time()
adaboost_fit_orig <-  train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "adaboost", 
                 trControl = fitControlrf,
                 tuneGrid = grid_ada,
                 metric='ROC'
)
proc.time() -  t0


saveRDS(adaboost_fit_orig, '../models/adaboost_fit_orig.RDS')

```

```{r, include=FALSE}
adaboost_fit_orig <- readRDS('../models/adaboost_fit_orig.RDS')
```

```{r}
adaboost_fit_orig
```



## Comparando los modelos generados

Ahora, generemos un modelo de benchmark: digamos, un modelo que predice solamente en función de la moda de la variable dependiente:

```{r}
dummy_pred <- function(y){
        uniqv <- unique(y)
        m<-uniqv[which.max(tabulate(match(y, uniqv)))]
        return(rep.int(m, length(y)))
}

eval_dummy <- function(obs){
        preds <- dummy_pred(obs)
        c <- confusionMatrix(preds, obs) 
        dummy <- c(c$overall[1], c$byClass)
        dummy <- as.data.frame(dummy) %>% t()
        return(dummy)
} 

```


Volvamos a armar la lista de modelos:

```{r}
model_list <- list(cart = cart,
                   rf_original = rf_fit_orig,
                   rf_weighted = rf_fit_wei,
                   rf_down = rf_fit_down,
                   rf_up = rf_fit_up,
                   adaboost = adaboost_fit_orig)



```


Y, finalmente, generemos una tabla que contenga todo:

```{r}
model_metrics <- model_list %>%
        map(extract_conf_metrics, data=test, obs = test$imp_inglab1) %>%
        do.call(rbind,.) %>%
        as.data.frame() %>%
        rbind(., eval_dummy(test$imp_inglab1)) %>%
        t()

model_metrics
```

Puede verse que cart, rf_original y adaboost presentan valores similares en accuracy. A su vez, al observar la relación entre precision y recall puede verse que el modelo que mejor balancea los dos es rf_up. Cuál elegir dependerá del objetivo del análisis.
